<!DOCTYPE html>
<html lang="en">

<head>
    <%- include("../partials/head"); %>
</head>

<body>
    <%- include("../partials/navbar"); %>

        <div class="container">
            <div class="row">
                <div class="col">
                    <h1 class="display-4">Simple Linear Regression: The comprehensive guide</h1>
                </div>
            </div>
            <hr class="my-4">
            <div class="row">
                <div class="col">
                    <p class="lead">Introduction</p>
                </div>
            </div>

            <p>
                This entry will attempt to give the reader an in-depth dive into simple linear regression (SLR).
                This
                includes the mathematics and
                the main assumptions behind it, as well as how to interpret the results. SLR in a nutshell is a
                predictive
                model, and like all other predictive models,
                its main purpose is to try and:
            </p>
            <ul>
                <li>Quantify the dependencies between some observed phenomena</li>
                <li>Predict their future behavior</li>
            </ul>
            <p>
                What separates SLR from other models is that SLR attempts to achieve this with a straight line.
                It
                is one of
                the oldest and simplest <span class="badge badge-primary" data-toggle="tooltip" data-placement="top"
                    title="Assumes that the data follows a certain statistical distribution">parametric</span>
                statistical models
                (or <span class="badge badge-primary" data-toggle="tooltip" data-placement="top"
                    title="Models trained on datasets with a dependant variable">supervised</span> machine
                learning
                algorithms), and forms the basis for many more complex ones.
                By understanding the inner workings and shortcomings of SLR, one would have a good foundation to
                expand their knowledge to other machine learning algorithms,
                as well as apply the concepts learnt in SLR to implement best practices in future modelling
                exercises. This article is broken up into several distinct sections, and depending on your level, you
                can choose
                to skip some of the refresher content.
            </p>
            <div class="accordion" id="straightLine">
                <div class="card">
                    <div class="card-header" id="straightLine">
                        <h2 class="mb-0">
                            <button class="btn btn-link" type="button" data-toggle="collapse" data-target="#collapseOne"
                                aria-expanded="true" aria-controls="collapseOne">
                                Refresher: What is a straight line?
                            </button>
                        </h2>
                    </div>
                    <div id="collapseOne" class="collapse" aria-labelledby="assumptions" data-parent="#straightLine">
                        <div class="card-body">
                            <p>
                                Given that the main feature of SLR models use a straight line to model phenomena, it
                                would be a good
                                idea to revisit the idea
                                of what a line is mathematically. If you can recall from high school, a line in
                                2-dimensional space can
                                be defined as:

                                $$ { y = mx + b } $$

                                Here, y and x are the horizontal and vertical coordinates, m is the slope of the line,
                                and b is the
                                intercept.
                                The intercept is basically where the line crosses the y-axis. The slope, or the gradient
                                of the line,
                                can be calculated as:
                                $${ m = \frac{ y_2 - y_1 }{ x_2 - x_1 } }$$

                                Now with these parameters, we can construct any possible 2-dimensional line.

                            </p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="row">
                <div class="col">
                    <p class="lead">The Regression model</p>
                </div>
            </div>

            The simple linear regression model can be defined as follows:

            $${ y_i = \beta_0 + \beta_1 x_i + \epsilon }$$

            We can see that this closely resembles an equation of a straight line, with an extra \(\epsilon\) term. This
            makes perfect sense
            since the goal of a SLR model is to model some data with a line in the first place. The \(x_i\) and \(y_i\)
            terms here represent our datapoints and
            response values respectively. The \(\beta_0\) and \(\beta_1\) term, which defines the intercept and slope of
            the line. While we note that these
            2 terms can take any numerical value and we'd still create a line, what we really want to do is to find the
            values for these 2 terms that creates
            the "best" line to approximate our data, or the \(x_i\) and \(y_i\) values. This is done by using the "Least
            Squares Method".
            <div class="accordion" id="assumptions">
                <div class="card">
                    <div class="card-header" id="assumptions">
                        <h2 class="mb-0">
                            <button class="btn btn-link" type="button" data-toggle="collapse" data-target="#collapseTwo"
                                aria-expanded="true" aria-controls="collapseOne">
                                Linear Regression Assumptions
                            </button>
                        </h2>
                    </div>
                    <div id="collapseTwo" class="collapse" aria-labelledby="assumptions" data-parent="#assumptions">
                        <div class="card-body">
                            <p class="lead">1. Residuals are normally distributed</p>
                            <p class="lead">2. Variance of residuals are constant</p>
                            <p class="lead">3. Residuals are indepdendent of each other</p>
                        </div>
                    </div>
                </div>
            </div>
            <div class="row">
                <div class="col">
                    <p class="lead">Least Squares Method</p>
                </div>
            </div>
            <p>
                The idea behind the least squares method, is to find values of \(\beta_0\) and \(\beta_1\), which
                minimizes the errors between the actual
                \(x_i\) and \(y_i\) terms observed in the dataset, and the values produced by the linear model. If you
                are mathematically savvy, you'd have picked up
                that this translates into an <span class="badge badge-primary" data-toggle="tooltip"
                    data-placement="top"
                    title="A class of problems relating to finding a max or min value of a function">optimization</span>
                problem.
            </p>
            <div class="accordion" id="widgetOne">
                <div class="card">
                    <div class="card-header" id="headingOne">
                        <h2 class="mb-0">
                            <button class="btn btn-link" type="button" data-toggle="collapse"
                                data-target="#collapseThree" aria-expanded="true" aria-controls="collapseOne">
                                Refresher: First Order Conditions
                            </button>
                        </h2>
                    </div>
                    <div id="collapseThree" class="collapse" aria-labelledby="headingOne" data-parent="#widgetOne">
                        <div class="card-body">
                            <p>
                                Optimization problems in mathematics are a class of problems where, given some
                                mathematical function \(f(x)\),
                                we want to find the point \(x\) such that \(f(x)\) returns the highest or lowest
                                possible value. In order to do this, we need to find the point where:
                                $${ \frac{\partial y}{\partial x} = f'(x) = 0 }$$
                                Or in other words, we need to find the point in the graph, where the <span
                                    class="badge badge-primary" data-toggle="tooltip" data-placement="top"
                                    title="The derivative of a function of a real variable measures the sensitivity to change of the function value with respect to a change in its argument">first
                                    derivative</span> (or slope)
                                has a flat gradient. If we refer to the function below where \(f(x) = x^2\), we note
                                that its derivate, or slope is defined as
                                \(f'(x) = 2x\) as shown on the right. If we followed along with the logic and set the
                                slope to 0, we get:
                                $${ 2x = 0, x = 0}$$
                                Which implies that when \(x = 0\), the function \(x^2\) will be at the lowest value.
                                Have a play around with the widget below and see if the 0 point on the \(f'(x)\) does
                                indeed
                                correspond to the lowest point on the \(f(x)\) graph on the left.
                            </p>
                            <div class="row">
                                <div class="col">
                                    <input type="range" class="slider" value="" min="-40" max="38" step="2"
                                        id="foc_x"><label for="foc_x">\(x\)</label>
                                </div>
                            </div>
                            <div class="row">
                                <div class="col-sm-6">
                                    <canvas id="firstOrderWidgetOne" width="200" height="200"></canvas>
                                </div>
                                <div class="col-sm-6">
                                    <canvas id="firstOrderWidgetTwo" width="200" height="200"></canvas>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <p>
                Here, the function that we are trying to minimize is actually embedded in the name of the method, least
                squares! We're trying to
                find values \(\beta_0\) and \(\beta_1\) that returns us the least amount of:
                $${ (y_{predicted} - y_{actual})^2 }$$
            </p>
            <div class="accordion" id="leastSquaresProof">
                <div class="card">
                    <div class="card-header" id="headingOne">
                        <h2 class="mb-0">
                            <button class="btn btn-link" type="button" data-toggle="collapse"
                                data-target="#collapseFour" aria-expanded="true" aria-controls="collapseOne">
                                Least Squares mathematical derivation
                            </button>
                        </h2>
                    </div>
                    <div id="collapseFour" class="collapse" aria-labelledby="headingOne"
                        data-parent="#leastSquaresProof">
                        <div class="card-body">
                            $${ Error = f(y) = \Sigma_i (y_{i} - \hat{y}_{i})^2 }$$
                            $${ Error = f(\beta_0, \beta_1) = \Sigma_i (y_{i} - (\beta_0 x_i + \beta_1))^2 }$$
                            $${ Error = f(\beta_0, \beta_1) = \Sigma_i (y_{i} -\beta_0 x_i - \beta_1)^2 }$$

                            Here, our function (whatever shape it has) output would change with different values of 'm'
                            and 'b' and it is our job to find the values of 'm' and 'b' that give the smallest output,
                            or the smallest error. This can be done with the concept of the zero-slope point discussed
                            above. We first take the partial derivatives with respect to both 'm' and 'b'.

                            $${ \frac{\partial f(\beta_0, \beta_1)}{\partial \beta_0} = 2\Sigma_i (y_{i} - \beta_1 x_i -
                            \beta_0)}$$
                            $${ \frac{\partial f(\beta_0, \beta_1)}{\partial \beta_1} = 2x_i \Sigma_i (y_{i} - \beta_1
                            x_i - \beta_0)}$$

                            We know that the minimum value for the error will occur when both:
                            $${ \frac{\partial f(\beta_0, \beta_1)}{\partial \beta_0} = 0 }$$
                            $${ \frac{\partial f(\beta_0, \beta_1)}{\partial \beta_1} = 0 }$$

                            Which means that we now have 2 equations:

                            $${ 2\Sigma_i (y_{i} - \beta_1 x_i - \beta_0) = 0}$$
                            $${ 2x_i \Sigma_i (y_{i} - \beta_1 x_i - \beta_0) = 0}$$

                            Which can be solved algbraically or with a system of equations, to give us the formulae:

                            $${ \beta_1 = \frac{\Sigma_i x_i \Sigma_i y_i - n \Sigma_i x_i y_i }{(\Sigma_i x_i)^2 -
                            n\Sigma_i (x_i)^2} }$$
                            $${ \beta_0 = \frac{1}{n}(\Sigma_i y_i - \beta_1 \Sigma_i x_i) }$$
                        </div>
                    </div>
                </div>
            </div>
            <p>
                By applying the concepts of optimizing the squared errors and some algebraic manipulation, we end up
                with the formulae:
                $${ \beta_1 = \frac{\Sigma x_i \Sigma y_i - n \Sigma x_i y_i }{(\Sigma x_i)^2 - n\Sigma (x_i)^2} }$$
                $${ \beta_0 = \frac{1}{n}(\Sigma y_i - \beta_1 \Sigma x_i) }$$

                While this seems somewhat cryptic at first, if we take a closer look at the results, we see that each of
                the terms can actually be
                calculated from our dataset.
            </p>
            <div class="row">
                <div class="col">
                    <input type="range" class="slider" value="" min="0.0" max="4.0" step="0.5" id="intercept"><label
                        for="intercept">Intercept</label>
                </div>
            </div>
            <div class="row">
                <div class="col">
                    <input type="range" class="slider" value="" min="0.0" max="1.0" step="0.1" id="slope"><label
                        for="intercept">Slope</label>
                </div>
            </div>
            <div class="row">
                <div class="col-sm-6">
                    <table class="table">
                        <thead class="thead-dark">
                            <tr>
                                <th scope="col">\(x_{i}\)</th>
                                <th scope="col">\(y_{i}\)</th>
                                <th scope="col">best-fit line</th>
                                <th scope="col">selected line</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>0</td>
                                <td>0</td>
                                <td>@mdo</td>
                                <td>@mdo</td>
                            </tr>
                            <tr>
                                <td>1</td>
                                <td>2</td>
                                <td>@fat</td>
                                <td>@mdo</td>
                            </tr>
                            <tr>
                                <td>2</td>
                                <td>1</td>
                                <td>@twitter</td>
                                <td>@mdo</td>
                            </tr>
                            <tr>
                                <td>3</td>
                                <td>4</td>
                                <td>@mdo</td>
                                <td>@mdo</td>
                            </tr>
                            <tr>
                                <td>4</td>
                                <td>2</td>
                                <td>@mdo</td>
                                <td>@mdo</td>
                            </tr>
                            <tr>
                                <td>5</td>
                                <td>4</td>
                                <td>@mdo</td>
                                <td>@mdo</td>
                            </tr>
                            <tr>
                                <td>6</td>
                                <td>2</td>
                                <td>@mdo</td>
                                <td>@mdo</td>
                            </tr>
                            <tr>
                                <td>7</td>
                                <td>7</td>
                                <td>@mdo</td>
                                <td>@mdo</td>
                            </tr>
                            <tr>
                                <td>8</td>
                                <td>5</td>
                                <td>@mdo</td>
                                <td>@mdo</td>
                            </tr>
                            <tr>
                                <td>9</td>
                                <td>6</td>
                                <td>@mdo</td>
                                <td>@mdo</td>
                            </tr>
                        </tbody>
                    </table>

                </div>
                <div class="col-sm-6">
                    <canvas id="leastSquaresWidget" width="200" height="200"></canvas>
                </div>
            </div>

            <div class="row">
                <div class="col">
                    <p class="lead">How to interpret results?</p>
                </div>
            </div>
            <p>
                First off, let's discuss what the a SLR result looks and the kind of statements we can make from it.
                From there, we can start to discuss the other
                statistics from a typical SLR model and the kind of statements we can make about those. From the example
                above, our
                SLR model looks like this:
                $${ y_i = 0.54 + 0.61 x_i + \epsilon }$$

                A common misconception is saying that "if my observation \(x\) is 1, then \(y\) would be 1.5". While the
                math does add up, the statement is inaccurate and does not reflect what the model
                is inferring. To be more accurate, given the assumptions we have made, the model actually tells us that
                \(y_i \sim{} N(\beta_0 + \beta_1 x_i, \sigma)\). This means that if we theoretically
                repeat the scenario where \(x = 1\) many times, our model is telling us that:
            <ol>
                <li>\(y\) will probably take different values on each scenario, but is expected to average out to 1.5
                </li>
                <li>The frequency of the different values \(y\) can take across the scenarios can be approximated by a
                    bell curve centered at 1.5</li>
                <li>This 1.5 figure will change depending on the observed value of \(x\), with the minimum of 0.54 when
                    \(x\) is 0</li>
                <li>We can be 95% confident that \(y\) will take values between \(1.5 \pm{} 1.645 *
                    \frac{\sigma}{\sqrt{n}} \)</li>
            </ol>
            </p>
            <div class="row">
                <div class="col">
                    <p class="lead">Validating the results</p>
                </div>
            </div>
            <p>
                Now that we've understood the main resulting formula and what the \(\beta_0\) and \(\beta_1\)
                coefficients mean, the SLR output
                provides us with more pieces of information which we can use to evaluate how "good" of a job our model
                is actually doing on
                approximating the data.
            </p>
            <p class="lead">P-Value</p>
            <p class="lead">Coefficient of determination</p>
            <p class="lead">Residual Plots</p>
            <div class="row">
                <div class="col">
                    <p class="lead">Limitations and uses</p>
                </div>
            </div>
            <p>
                While arguably not many real world relationships are linear in nature, SLR still proves useful for
                short term approximations like:
            <ul>
                <li>Revenue and ad spending</li>
                <li>Drug dosage and blood pressure</li>
                <li>Crop yield and water + fertilizer amount</li>
                <li>Player performance in sports and training regime hours</li>
            </ul>
            Furthermore, the concepts in SLR are applied to various more complex models. For example, explanation model
            LIME
            uses a local linear model in an attempt to explain more complex machine learning results. A feed forward
            neural network
            with 1 bias node and 1 weight node is essentially a SLR model given squared errors and the identity
            activation function.
            </p>
        </div>
</body>
<script src="/js/slr.js"></script>
<script src="/js/slr_poc.js"></script>

</html>